{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b47498b-6cfd-430d-ab47-8891b9440a1f",
   "metadata": {},
   "source": [
    "### Autoplan to get Strategies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7f1411-e4dc-4552-8817-9379e68f62c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json \n",
    "import regex\n",
    "import random\n",
    "import pickle \n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import openai\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    " \n",
    "# Create and configure logger\n",
    "logging.basicConfig(filename=\"Autoplan.log\",\n",
    "                    format='%(asctime)s %(message)s',\n",
    "                    filemode='w')\n",
    " \n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    " \n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "def pretty_print(role, text, verbose=False):\n",
    "    string = '------------{}-----------\\n{}\\n'.format(role, text)\n",
    "    if verbose: print(string, end=''), sys.stdout.flush()\n",
    "    return string\n",
    "\n",
    "def Calculate(expression):\n",
    "    expression = expression.replace(\"^\", \"**\")\n",
    "    return eval(expression)    \n",
    "\n",
    "\n",
    "class HotpotQA:\n",
    "    def __init__(self, lm, method, backend_args, quota_args, **kwargs):\n",
    "        self.lm = lm\n",
    "        self.method = method  # step\n",
    "        self.backend = backend_args['name']  # openai\n",
    "        \n",
    "        if self.backend == 'openai':\n",
    "            openai.api_type = \"azure\"\n",
    "            openai.api_base = \"https://mrityunjoypanday-gpt4.openai.azure.com/\"\n",
    "            openai.api_version = \"2023-03-15-preview\"\n",
    "            openai.api_key = \"d020880e0119447fbde26d3dcfb09bd7\"\n",
    "    \n",
    "        self.top_p = backend_args['top_p']\n",
    "        self.temp = backend_args['temp']\n",
    "        self.max_token = backend_args['max_token']\n",
    "        self.presence_penalty = backend_args['presence_penalty']        \n",
    "        \n",
    "        self.max_iter_per_instance = quota_args['max_iter_per_instance']\n",
    "\n",
    "        \n",
    "        self.history = []\n",
    "        self.strategy = None\n",
    "\n",
    "        # openai api\n",
    "        self.n_prompt_token = 0\n",
    "        self.n_sample_token = 0\n",
    "        self.messages = []\n",
    "        \n",
    "    def task_desription(self):\n",
    "        return '''Solve a Math question answering task through actions(if required) which can be of following types: \n",
    "(1) calculator[expression]: Stricly use only for simple numerical calculations which can be calculated by Pythons eval function. (Do not use this function if any variable(alphabet) is encountered in the expression; Instead use LLM action). \n",
    "                            Correct Use: calculator[(3*4)/9]\n",
    "                                         Calculator[4+5+9-2]\n",
    "                            Wrong use: calculator[smallest positive integer that ends in 9 and is divisible by 7]\n",
    "                                       calculator[20 divided by 2]\n",
    "                            \n",
    "(2) finish[answer]: return the answer and finish the task.\n",
    "(3) LLM[string]: get the answer using LLM.'''\n",
    "    \n",
    "    def score(self, instance, prediction):\n",
    "        question, answer = instance['question'], instance['answer']\n",
    "        query = \"\"\"Question: \"{}\"\\n\n",
    "Gold Answer: \"{}\"\\n\n",
    "My Prediction: \"{}\"\\n\n",
    "Verify whether my prediction to the question is equivalent to gold answer. Respond with yes/no.\"\"\".format(question, answer, prediction)\n",
    "\n",
    "        message = {\n",
    "            'role': 'user',\n",
    "            'content': query\n",
    "        }\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                response = func_timeout(10, \n",
    "                    openai.ChatCompletion.create,\n",
    "                    kwargs={\n",
    "                        \"engine\": \"gpt-4\",\n",
    "                        \"messages\": [message],\n",
    "                        \"temperature\": 0.\n",
    "                    }                \n",
    "                )['choices'][0]['message']['content'].lower()\n",
    "                break\n",
    "            except FunctionTimedOut:\n",
    "                print('OpenAI API call timeout')\n",
    "                continue\n",
    "        \n",
    "        assert 'yes' in response or 'no' in response, (question, answer, prediction, response)\n",
    "        return 'yes' in response\n",
    "    \n",
    "    def eval(self, instances, predictions):\n",
    "        n_correct = 0\n",
    "        correct_mask = []\n",
    "        for idx in tqdm(range(len(instances)), desc='Evaluating'):\n",
    "            result = self.score(instances[idx], predictions[idx])\n",
    "            n_correct += int(result)\n",
    "            correct_mask.append(result)\n",
    "        \n",
    "        return {'n_correct': n_correct, 'correct_mask': correct_mask}\n",
    "\n",
    "    \n",
    "    def formalize(self, action):\n",
    "        message = \"\"\"Valid action formats are as follows:\n",
    "(1) calculator[expression]\n",
    "(2) finish[answer]\n",
    "(3) LLM[string]\n",
    "\n",
    "Formalize the following action strictly into the above valid action formats. If there are multiple actions, formalize the first one.\n",
    "\n",
    "Action: I want to do a calculation of  (45 * 95) / 5. \n",
    "Formalized First Action: calculator[(45*95)/5]\n",
    "\n",
    "Action: i got the final answer as 90.\n",
    "Formalized First Action: finish[90]\n",
    "\n",
    "Action: get the answer for \"n=5*k+3. find the possible values for n\"\n",
    "Formalized First Action: LLM[n=5*k+3. find the possible values for n]\n",
    "\n",
    "Action: {}\n",
    "Formalized First Action:\n",
    "\"\"\".format(action)\n",
    "        message = {'role': 'user', 'content': message}\n",
    "        action = self.call_openai_api([message], stop='\\n')['choices'][0]['message']['content']\n",
    "\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def recognize(self, response):\n",
    "\n",
    "        calculator = regex.search(r'calculator\\[(.*)\\]', response)\n",
    "        Solve = regex.search(r'LLM\\[(.*)\\]', response)\n",
    "        finish = regex.search(r'finish\\[(.*)\\]', response)\n",
    "\n",
    "        max_start = max([x.start() for x in [calculator, Solve, finish] if x is not None], default=-1)\n",
    "        if max_start == -1:\n",
    "            match = None\n",
    "        elif calculator and max_start == calculator.start():\n",
    "            match = calculator\n",
    "        elif Solve and max_start == Solve.start():\n",
    "            match = Solve\n",
    "        elif finish and max_start == finish.start():\n",
    "            match = finish\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return match, calculator, Solve, finish\n",
    "    \n",
    "    def price(self):\n",
    "        price = PRICE[self.lm]\n",
    "        return (self.n_prompt_token * price['prompt'] + self.n_sample_token * price['sample']) / 1000\n",
    "    \n",
    "    def call_openai_api(self, messages, stop, lm=None, top_p=None):\n",
    "        n_try = 10\n",
    "        while n_try > 0:\n",
    "            try:\n",
    "                time.sleep(1)\n",
    "                response = func_timeout(90, \n",
    "                    openai.ChatCompletion.create,\n",
    "                    kwargs={\n",
    "                        \"engine\": self.lm if lm is None else lm,\n",
    "                        \"messages\": messages,\n",
    "                        \"top_p\": self.top_p if top_p is None else top_p,\n",
    "                        \"temperature\": self.temp,\n",
    "                        \"max_tokens\": self.max_token,\n",
    "                        \"presence_penalty\": self.presence_penalty,\n",
    "                        \"stop\": stop,\n",
    "                    }\n",
    "                )\n",
    "                break\n",
    "            except FunctionTimedOut:\n",
    "                print('[LOG] OpenAI API call timeout')\n",
    "                n_try -= 1\n",
    "                if n_try == 0:\n",
    "                    raise Exception('Failed 10 retries.')\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                #print('[LOG]', e)\n",
    "                time.sleep(15)\n",
    "                n_try -= 1\n",
    "                if n_try == 0:\n",
    "                    raise Exception('Failed 10 retries.')\n",
    "                continue\n",
    "        return response\n",
    "        \n",
    "    def call_lm(self, prompt, add_response=True, stop=None, lm=None, top_p=None):\n",
    "        \n",
    "        self.messages.append({'role': 'user', 'content': prompt})\n",
    "        response = self.call_openai_api(self.messages, stop, lm=lm, top_p=top_p)\n",
    "        if add_response: self.messages.append(response['choices'][0]['message'])\n",
    "        self.n_prompt_token += response['usage']['prompt_tokens']\n",
    "        self.n_sample_token += response['usage']['completion_tokens']\n",
    "        return response['choices'][0]['message']['content']\n",
    "    \n",
    "    def LLM(self, string):\n",
    "        messages = [{'role': 'user', 'content': str(string)}]\n",
    "        response = self.call_openai_api(messages, stop=None, lm=None, top_p=None)\n",
    "        return response['choices'][0]['message']['content']\n",
    "        \n",
    "    def init(self):\n",
    "        prompt = \"{}\\n\\nTask Plan:\".format(self.task_desription())\n",
    "        strategy = self.call_lm(prompt)\n",
    "        return strategy\n",
    "    \n",
    "    \n",
    "    def run(self, data, strategy=None, is_test=False, verbose=False, return_history=False):\n",
    "        questions = []\n",
    "        answers = []\n",
    "        predictions = []\n",
    "        summaries = []\n",
    "        flawed_actions = []\n",
    "        flawed_plans = []\n",
    "        history = ''\n",
    "\n",
    "        \n",
    "        for q_idx in range(len(data)):\n",
    "            self.messages = []\n",
    "            init_msg = self.task_desription()\n",
    "            if 'direct' not in self.method and strategy is not None:\n",
    "                init_msg += \"\\n\\nTask Plan:\\n{}\\n\".format(strategy)\n",
    "            init_msg += '\\n'\n",
    "\n",
    "            question = data[q_idx]['question']\n",
    "            answer = data[q_idx]['answer']\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "\n",
    "            # print(question, answer, sep='\\n')\n",
    "\n",
    "            if 'step' in self.method and strategy is not None:\n",
    "                thought_prompt = \"Identify which step of plan you are at. Show your thought about the one next action. Your thought should be faithful to the plan step.\"\n",
    "            else:\n",
    "                thought_prompt = \"Show your thought about the next action.\"\n",
    "\n",
    "            input_msg = init_msg + '\\nQuestion: ' + question + '\\n' + thought_prompt\n",
    "            history += pretty_print('Human', input_msg, verbose)\n",
    "            logger.info(pretty_print('Human', input_msg, verbose))\n",
    "            \n",
    "            cur_itr = 0\n",
    "            has_answer = False\n",
    "            while cur_itr < self.max_iter_per_instance:\n",
    "                \n",
    "                # Thought\n",
    "                thought = self.call_lm(input_msg, stop='\\n')\n",
    "                history += pretty_print('Machine', thought, verbose)\n",
    "                logger.info(pretty_print('Machine', thought, verbose))\n",
    "                # Action\n",
    "                \n",
    "                action = self.call_lm('Action:', add_response=False, stop='\\n')\n",
    "                history += pretty_print('Human', f'Action:{action}', verbose)\n",
    "                formalized_action = self.formalize(action)\n",
    "                self.messages.append({'role': 'assistant', 'content': formalized_action})\n",
    "                history += pretty_print('Machine', '{} (orig {})'.format(formalized_action, action), verbose)\n",
    "                logger.info(pretty_print('Machine', '{} (orig {})'.format(formalized_action, action), verbose))\n",
    "                \n",
    "                rec_res, m_calculator, m_solve, m_final = self.recognize(formalized_action)\n",
    "\n",
    "                cmd, object = None, None\n",
    "                if rec_res:\n",
    "                    cmd = rec_res.group(0)\n",
    "                    object = rec_res.group(1).strip('\"\\'')\n",
    "                \n",
    "                if rec_res:\n",
    "                    input_msg = 'Observation:\\n'\n",
    "                    if rec_res == m_calculator:\n",
    "                        #self.wiki.search_step(object)\n",
    "                        expression_res = Calculate(object)\n",
    "                        input_msg += str(expression_res)\n",
    "                    elif rec_res == m_solve:\n",
    "                        res = self.LLM(object)\n",
    "                        #print(\"LLM: \", type(res), res)\n",
    "                        input_msg += res\n",
    "                    else:\n",
    "                        predictions.append(object)\n",
    "                        has_answer = True\n",
    "                        break\n",
    "                else:\n",
    "                    input_msg = random.choice([\n",
    "                        \"No action in the format of 'Calculator[...]' or 'finish[...]'.\",\n",
    "                    ])\n",
    "\n",
    "                if cur_itr < self.max_iter_per_instance - 1:\n",
    "                    input_msg += '\\n' + thought_prompt\n",
    "                history += pretty_print('Human', input_msg, verbose)\n",
    "                logger.info(pretty_print('Human', input_msg, verbose))\n",
    "                \n",
    "                cur_itr += 1\n",
    "\n",
    "            if not has_answer:\n",
    "                predictions.append('')\n",
    "            \n",
    "            if not is_test:\n",
    "                if not has_answer:\n",
    "                    summary_msg = 'Max number of iteration reached. No answer is found.'\n",
    "                else:\n",
    "                    summary_msg = 'Task finished.'\n",
    "\n",
    "#                 supporting_entities = set()\n",
    "#                 for f in data[q_idx]['supporting_facts']:\n",
    "#                     supporting_entities.add('\"{}\"'.format(f[0]))\n",
    "#                 supporting_entities = list(supporting_entities)\n",
    "                \n",
    "#                 supporting_string = None\n",
    "#                 if len(supporting_entities) == 1:\n",
    "#                     supporting_string = supporting_entities[0]\n",
    "#                 else:\n",
    "#                     supporting_string = ', '.join(supporting_entities[:-1]) + ' and ' + supporting_entities[-1]                \n",
    "\n",
    "                summary_msg += ' The ground truth answer is \"{}\". Summarize the interaction history concisely.'.format(answer)\n",
    "\n",
    "                history += pretty_print('Human', summary_msg, verbose)\n",
    "                logger.info(pretty_print('Human', summary_msg, verbose))\n",
    "                \n",
    "                summary = self.call_lm(summary_msg, top_p=0.)\n",
    "                history += pretty_print('Machine', summary, verbose)\n",
    "                logger.info(pretty_print('Machine', summary, verbose))\n",
    "                \n",
    "                summaries.append(summary)\n",
    "                \n",
    "                if strategy is not None:\n",
    "\n",
    "                    failed_action_msg = 'Identify all flawed parts of the plan (not flawed action). Only the flawed part.'\n",
    "                    history += pretty_print('Human', failed_action_msg, verbose)\n",
    "                    logger.info(pretty_print('Human', failed_action_msg, verbose))\n",
    "                    \n",
    "                    failed_action = self.call_lm(failed_action_msg, top_p=0.)\n",
    "                    history += pretty_print('Machine', failed_action, verbose)\n",
    "                    logger.info(pretty_print('Machine', failed_action, verbose))\n",
    "                    \n",
    "                    \n",
    "                    flawed_actions.append(failed_action)\n",
    "\n",
    "                    suggest_rev_msg = 'Suggest revision to the current flawed part of the plan. Only the flawed part.'\n",
    "                    history += pretty_print('Human', suggest_rev_msg, verbose)\n",
    "                    logger.info(pretty_print('Human', suggest_rev_msg, verbose))\n",
    "                    \n",
    "                    suggest_rev = self.call_lm(suggest_rev_msg, stop=None, top_p=0.0)\n",
    "                    history += pretty_print('Machine', suggest_rev, verbose)\n",
    "                    logger.info(pretty_print('Machine', suggest_rev, verbose))\n",
    "                    \n",
    "                    flawed_plans.append(suggest_rev)\n",
    "                    \n",
    "            print(\"\\n\\n############\\n\\nID Done:\", q_idx, \"\\n\\n\")\n",
    "            \n",
    "            with open(\"summaries.json\", 'w') as f:\n",
    "                json.dump({\"questions\":questions, \"answers\":answers, \"summaries\": summaries, \"flawed_actions\": flawed_actions, \"flawed_plans\":flawed_plans}, f)\n",
    "            \n",
    "\n",
    "        to_return = None\n",
    "\n",
    "        if is_test:\n",
    "            to_return = predictions\n",
    "        else:\n",
    "            self.messages = []\n",
    "            final_msg = 'Task Description:\\n' + self.task_desription() + '\\n\\n'\n",
    "            final_msg += 'Current Task Plan:\\n{}\\n\\n'.format(strategy)\n",
    "            final_msg += '=' * 10 + 'Task Experiences Begin' + '=' * 10 + '\\n\\n'\n",
    "\n",
    "            \n",
    "            for q_idx in range(len(data)):\n",
    "                question = data[q_idx]['question']\n",
    "                final_msg += 'Job {}: Answering the following question. {}\\nSummary of Job {}:\\n{}\\n'.format(q_idx, question, q_idx, summaries[q_idx])\n",
    "                if strategy is not None:\n",
    "                    final_msg += 'Flaws of Plan in Job {}:\\n{}\\n'.format(q_idx, flawed_actions[q_idx])\n",
    "                    final_msg += 'Suggested Revision of Plan from Job {}:\\n{}\\n'.format(q_idx, flawed_plans[q_idx])\n",
    "\n",
    "            final_msg += '=' * 10 + 'Task Experiences End' + '=' * 10 + '\\n\\n'\n",
    "\n",
    "            final_msg += 'Based on the above {} experiences of the task, rewrite the current task plan. The plan should not be specific to one job but generalizable to all jobs. \\nNew Task Plan:'.format(len(data))\n",
    "\n",
    "            history += pretty_print('Human', final_msg, verbose)\n",
    "            logger.info(pretty_print('Human', final_msg, verbose))\n",
    "            \n",
    "            with open(\"final_msg.txt\", \"w\") as f:\n",
    "                f.write(final_msg)\n",
    "                \n",
    "            new_strategy = self.call_lm(final_msg, top_p=0.)\n",
    "            \n",
    "            \n",
    "            history += pretty_print('Machine', new_strategy, verbose)\n",
    "            logger.info(pretty_print('Machine', new_strategy, verbose))\n",
    "            \n",
    "            to_return = new_strategy\n",
    "\n",
    "        self.history.append(history)\n",
    "\n",
    "        if return_history:\n",
    "            to_return = to_return, history\n",
    "            return to_return\n",
    "\n",
    "        # if not is_test:\n",
    "        #     n_correct = self.eval(data, predictions)['n_correct']\n",
    "\n",
    "        return to_return, history\n",
    "    \n",
    "\n",
    "backend_args = {\n",
    "        'name': \"openai\",\n",
    "        'top_p': 1,\n",
    "        'temp': 0,\n",
    "        'max_token': 3000,\n",
    "        'presence_penalty': 1.5,\n",
    "    }\n",
    "\n",
    "quota_args = {\n",
    "        'sleep_minutes': 1,\n",
    "        'max_iter_per_instance': 4\n",
    "}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../MATH.csv\")\n",
    "df.head(1)\n",
    "\n",
    "data = []\n",
    "for level in [1, 2, 3, 4, 5]:\n",
    "    level = f\"Level {level}\"\n",
    "    new = df[df['level']==level].sample(5, random_state=42)\n",
    "    print(level, new.index)\n",
    "    for i in range(5):\n",
    "        q, l = new.iloc[i]['problem'], new.iloc[i]['label']\n",
    "        data.append({\"question\": q, \"answer\": l})\n",
    "        \n",
    "hotpot = HotpotQA(\"gpt-4\", 'step', backend_args, quota_args)\n",
    "\n",
    "batch_size = 5\n",
    "new_strategy = \"Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan and solve the problem step by step. \"\n",
    "for batch in range(0, len(data), batch_size):\n",
    "    new_strategy, history = hotpot.run(data[batch: batch + batch_size],\n",
    "                                    strategy=new_strategy,\n",
    "                                    is_test=False,\n",
    "                                    verbose=True,\n",
    "                                    return_history=True)\n",
    "    \n",
    "    with open(f'strategy_{batch}.txt', 'w') as f:\n",
    "        f.write(new_strategy)\n",
    "        s = \"\\n\\n\" + \"=\"*50 + \"\\n\\n\"\n",
    "        f.write(s)\n",
    "        f.write(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
