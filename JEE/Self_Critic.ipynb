{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef0f889-7606-4c28-ad62-05e8d3cc0fec",
   "metadata": {},
   "source": [
    "## JEE with Self-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5698ee-03c9-40b8-93e0-6f7c4ef9663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import trange, tqdm\n",
    "import os\n",
    "import openai\n",
    "import logging\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"\"\n",
    "openai.api_version = \"\"\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666da519-b675-400c-8426-ad0621408405",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/dataset.json\", 'r') as f:\n",
    "    l = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f73f21-55bf-4518-ba85-77a75a0af7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dic = {\n",
    "\"MCQ\": \"In this problem, only one option will be correct. Give a detailed solution and end the solution with the final answer.\",\n",
    "\"MCQ(multiple)\": \"In this problem, multiple options can be correct. Give a detailed solution and end the solution with the final answer.\",  \n",
    "\"Integer\": \"In this problem, the final answer will be a non-negative integer. Give a detailed solution and end the solution with the final answer.\",\n",
    "\"Numeric\": \"In this problem, the final will be a numeric value. Give the numerical answer correct upto the 2nd decimal digit. Give a detailed solution and end the solution with the final answer.\"\n",
    "}\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def gpt4(system, user_prompt):\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "      engine=\"gpt-4-32k\",\n",
    "      messages= [\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "      temperature=0,\n",
    "      max_tokens=4000,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "def reflection_prompt(input_text, logger):\n",
    "    system='''Act like a MIT professor having 20 years of teaching experience. You will first answer the question. Then you will criticize your answer and then finally you will give the correct response.\n",
    "    \n",
    "    Instructions :\n",
    "     You have to only give the correct option as the response\n",
    "     Give only final response in strictly JSON format only, with one key as \"explanation\" and the  explanation and the second key as \"answer\" and the answer.\n",
    "    '''\n",
    "    user_prompt=f'''\n",
    "    Solve the given MCQ: {input_text}\n",
    "    '''\n",
    "    result = gpt4(system, user_prompt)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "def get_json(result, logger, count=1):\n",
    "    \n",
    "    try:\n",
    "        # getting only final Json.\n",
    "        system='''\n",
    "        Give response in strictly JSON format only, with one key as \"explanation\" and the  explanation and the second key as \"answer\" and the answer.\n",
    "        '''\n",
    "        user_prompt=f'''\n",
    "        Extract Final JSON from:\n",
    "\n",
    "        {result}\n",
    "        '''\n",
    "        result_ = gpt4(system, user_prompt)\n",
    "        \n",
    "        return json.loads(result_)\n",
    "    \n",
    "    except:\n",
    "        if count+1 > 5:\n",
    "            return json.loads('{\"explanation\": \"Something went wrong\", \"answer\": \"Wrong\"}')\n",
    "        \n",
    "        return get_json(result, logger, count+1)\n",
    "\n",
    "\n",
    "def get_prompt(question: dict):\n",
    "    p = prompts_dic[question['type']]\n",
    "    return p + \"\\n\\n\" +  question['question'], question['gold']\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "\"Index\":[],\n",
    "\"Question\":[],\n",
    "\"Gold\":[],\n",
    "\"first_answer\":[],\n",
    "\"explanation\": [],\n",
    "\"Answer\":[],\n",
    "\"subject\":[],\n",
    "\"type\":[]\n",
    "})\n",
    "\n",
    "for i in trange(0, len(l)):\n",
    "    \n",
    "    try:\n",
    "        print(\"Execution: \", i)\n",
    "\n",
    "        q, a = get_prompt(l[i])\n",
    "\n",
    "        x = reflection_prompt(q, logger)\n",
    "        t = get_json(x, logger, 1)\n",
    "\n",
    "        df.loc[len(df)] = (l[i]['index'], l[i]['question'], l[i]['gold'], x, t[\"explanation\"], t['answer'], l[i]['subject'], l[i]['type'])\n",
    "\n",
    "        df.to_excel(\"./output/Jee_Self_Critic_output.xlsx\", index=False)\n",
    "        \n",
    "    except:\n",
    "        print(f\"Exception at {i}..\")\n",
    "    \n",
    "df.to_excel(\"./output/Jee_Self_Critic_output.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d52de5-0899-46a2-bef4-62d75554206d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1befb314-14fa-4892-bb4e-6d851c2b60a4",
   "metadata": {},
   "source": [
    "### Evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ec44629-707e-42af-bfd3-89eec683a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"./output/Jee_Self_Critic_Scores.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75c01ea-3131-40d9-82d5-7decd4f1cd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26222222222222225"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "757a9b21-afcc-46a0-9a28-279f5269c78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject\n",
       "chem    0.345324\n",
       "math    0.214461\n",
       "phy     0.245327\n",
       "Name: Score, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['subject'])['Score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de01cf5d-4a68-40e6-89ed-113d512a3ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "Integer          0.120690\n",
       "MCQ              0.433735\n",
       "MCQ(multiple)    0.267045\n",
       "Numeric          0.210526\n",
       "Name: Score, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['type'])['Score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83516e88-f5b5-4c60-a793-6a6e74e55c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funsearch",
   "language": "python",
   "name": "funsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
