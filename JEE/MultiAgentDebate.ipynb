{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd50824-7958-43ea-bb95-b2e8c8934c2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## JEE with MultiAgentDebate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd9a42-1178-4640-a8e2-d27e8038dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import trange\n",
    "import time\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "import openai\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"\"\n",
    "openai.api_version = \"\"\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c7a61-c38f-4495-b248-7365af0acb2a",
   "metadata": {},
   "source": [
    "#### MAD core code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1cf9bd-84df-4aa5-a67f-acc2c7910d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def gpt4(text):\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "          engine=\"gpt-4-32k\",\n",
    "          messages= [\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ],\n",
    "          temperature=0.7,\n",
    "          max_tokens=6000,\n",
    "          top_p=1,\n",
    "          frequency_penalty=0,\n",
    "          presence_penalty=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as exc:\n",
    "        print(exc)\n",
    "        time.sleep(30)\n",
    "        return gpt4(text)\n",
    "    \n",
    "\n",
    "def get_res_from_other_agents(gpt4_res, get_prompt_for_agent):\n",
    "    \n",
    "    final_prompt = \"These are responses from other agents.\"\n",
    "    agent_c = 1\n",
    "    for i in range(len(gpt4_res)):\n",
    "        if i != get_prompt_for_agent:\n",
    "            final_prompt += f\"\\n Agent{agent_c} :\" + gpt4_res[i] \n",
    "            agent_c += 1\n",
    "    \n",
    "    final_prompt += \"\\n Based on the opinion of other agents , give an updated response\"\n",
    "    return final_prompt\n",
    "\n",
    "\n",
    "def output(init_prompt, no_of_agents):\n",
    "    \n",
    "    gpt4_res = [gpt4(init_prompt) for i in range(no_of_agents)]\n",
    "    \n",
    "    folder_name = f\"{no_of_agents}_Model_multi_agent_output\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    gpt4_1_file = os.path.join(folder_name, \"gpt4_1.txt\")\n",
    "    \n",
    "    ## Initial prompt\n",
    "    with open(gpt4_1_file, 'w') as file:\n",
    "        file.write(\"Initial prompt >>>>>>>> \"+ init_prompt)\n",
    "        file.write('*'*500)\n",
    "        file.write(\"\\n\\n\")\n",
    "    \n",
    "    for i in trange(1, 6):\n",
    "        gpt4_input = [get_res_from_other_agents(gpt4_res, i) for i in range(no_of_agents)]\n",
    "\n",
    "        gpt4_res = [gpt4(inp) for inp in gpt4_input]\n",
    "        \n",
    "        with open(gpt4_1_file, 'a') as file:\n",
    "            file.write(\"After iteration >>>>>>>>>>>>>>>> \"+ str(i))\n",
    "            file.write(\"\\n\")\n",
    "            file.write(gpt4_res[0])\n",
    "            file.write(\"\\n\")\n",
    "            file.write('~'*500)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    with open(gpt4_1_file, 'r') as file:\n",
    "        return_text = file.read()\n",
    "    \n",
    "    return gpt4_res[0], return_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7757db-2f74-4365-bdb8-0e1af5b59cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "407a5c08-a699-41bf-add4-c389104a6869",
   "metadata": {},
   "source": [
    "### Iterating over dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aef628-5914-4238-90f1-ec48a7d5b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from tqdm import trange\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"./data/dataset.json\", 'r') as f:\n",
    "    l = json.load(f)\n",
    "\n",
    "prompts_dic = {\n",
    "\"MCQ\": \"In this problem, only one option will be correct. Give a detailed solution and end the solution with the final answer.\",\n",
    "\"MCQ(multiple)\": \"In this problem, multiple options can be correct. Give a detailed solution and end the solution with the final answer.\",  \n",
    "\"Integer\": \"In this problem, the final answer will be a non-negative integer. Give a detailed solution and end the solution with the final answer.\",\n",
    "\"Numeric\": \"In this problem, the final will be a numeric value. Give the numerical answer correct upto the 2nd decimal digit. Give a detailed solution and end the solution with the final answer.\"\n",
    "}\n",
    "\n",
    "ans_dic = {\n",
    "\"MCQ\": \"A or B or C or D.\",\n",
    "\"MCQ(multiple)\": \"if correct options are A, B and D give ABD alphabetically.\",  \n",
    "\"Integer\": \"Int number only.\",\n",
    "\"Numeric\": \"Float number only.\"\n",
    "}\n",
    "\n",
    "def get_prompt(question: dict):\n",
    "    \n",
    "    p = prompts_dic[question['type']]\n",
    "    f_text = f\"\"\"\\n\\nGive final answer as Final_answer: {ans_dic[question['type']]}\"\"\"\n",
    "    \n",
    "    return p + \"\\n\\nProblem: \" + question['question'] + f_text + \"\\n\\nLetâ€™s think step by step\", question['gold']\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "'description':[],\n",
    "'index':[],\n",
    "'subject':[],\n",
    "'type':[],\n",
    "'question':[], \n",
    "'gold':[],\n",
    "'pred':[],\n",
    "\"all_iter\":[]\n",
    "})\n",
    "\n",
    "\n",
    "no_of_agents = 6\n",
    "for i in trange(len(l)):\n",
    "    \n",
    "    try:\n",
    "        q, a =  get_prompt(l[i])\n",
    "        res, all_res = output(q, no_of_agents)\n",
    "        l[i]['pred'] = res \n",
    "\n",
    "        df.loc[len(df)] = (l[i]['description'], l[i]['index'], l[i]['subject'], l[i]['type'], l[i]['question'], l[i]['gold'], l[i]['pred'], all_res)\n",
    "\n",
    "        if i% 1 == 0:\n",
    "            df.to_excel('Jee_MAD.xlsx', index=False)\n",
    "    \n",
    "    except Exception as exc:\n",
    "        print(f\"Exception {exc} at {i}\")\n",
    "        \n",
    "\n",
    "df.to_excel('./output/Jee_MAD_output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458943d-829b-4fa4-8402-f544e7adb97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "069fba74-380b-4eb5-9b7b-be173abd7939",
   "metadata": {},
   "source": [
    "### Scoring the Outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841dda8-89a3-4c5c-aff5-d8485107d113",
   "metadata": {},
   "source": [
    "Started with MAD after COT\n",
    "\n",
    "Working with 6 agents and 6 rounds of Debate. taking around (20-25min) (Should i increase agent count to 20?)\n",
    "\n",
    "Also i observe mostly first round results are carried forward to subsequent rounds(even though it is wrong solution.)\n",
    "\n",
    "Should we include the question as well for each round? \n",
    "currently for each round except 1st round, we are passing agents output only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9525544-e935-4e97-8e8f-4c4b4d07ce84",
   "metadata": {},
   "source": [
    "## JEEBench using Multi Agent Debate.\n",
    "\n",
    "1. Run MAD_run.py to generate the gpt output for all questions and saves in **Jee_MAD.xlsx**.\n",
    "2. Run Jee_MAD_Scoring.py to manually score using Paper eavaluation scheme the GPT4 generations comparing with GOLD and generate **Jee_MAD_Scores_paperScoring.xlsx**.\n",
    "3. Run Jee_MAD_Scoring.py again to manually score using JEE original exam scoring the GPT4 generations comparing with GOLD and generate **Jee_MAD_Scores+4.xlsx**.\n",
    "4. Run this notebook to see the scores for each Subject and Category."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d0d075c-f2a4-4cbd-9755-a1774d096763",
   "metadata": {},
   "source": [
    "Papaer Scoring is the same way the JEEBench paper has evaluated.\n",
    "\n",
    "MCQ:           correct-1 mark, wrong-0 mark\n",
    "MCQ(multiple): if even a single wrong option is selected we gave 0 marks.\n",
    "               if all correct options are selected 1 mark.\n",
    "               if one of the correct options is selected 0.25 mark.\n",
    "               if two of the correct options are selected 0.5 mark.\n",
    "               if three of the correct options are selected 0.75 mark.\n",
    "               \n",
    "int:           if it is exactly correct 1 mark else 0 mark.\n",
    "float:         if it is only difference of 0.01 from gold label then 1 mark else 0 mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703f9838-470f-4855-881c-676e9dc06a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject\n",
       "chem    0.433453\n",
       "math    0.281863\n",
       "phy     0.391509\n",
       "Name: Score, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"./JEE/output/Jee_MAD_Scores_paperScoring.xlsx\")\n",
    "df.groupby('subject')['Score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ddbb5c-8663-4459-bce7-4e95fd286c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "Integer          0.327586\n",
       "MCQ              0.475610\n",
       "MCQ(multiple)    0.370739\n",
       "Numeric          0.270677\n",
       "Name: Score, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('type')['Score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dcc4eac-8d4f-4644-911b-43758a8487a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35467706013363026"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a7cde-fb9d-4a46-9f19-7281e25f8043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a02cb-d9e6-433a-a1cf-bde18e9d1cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "dfe356c4-64f9-4a54-951a-f3e449f4abea",
   "metadata": {},
   "source": [
    "Exam Scoring of JEE.\n",
    "\n",
    "MCQ: correct: +3 \n",
    "     wrong: -1\n",
    "     unattemped: 0\n",
    "     \n",
    "MCQ(multi): +4 â€“ If the correct option(s) is selected.\n",
    "            +3 â€“ if only 3 options are chosen out pf the four correct answers.\n",
    "            +2 â€“ If only two options are chosen but three or more options are correct, both of which are correct options.\n",
    "            +1 â€“if only one option is chosen but more than 2 options are correct.\n",
    "            -2: In all other cases.\n",
    "            0: not attempted.\n",
    "\n",
    "int: (no negative)\n",
    "    Correct: +4\n",
    "    wrong: 0\n",
    "    \n",
    "Numeric: (no negative)\n",
    "    Correct: +4\n",
    "    wrong: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c0c50f-fba6-4511-8752-d3b36744d9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "subject\n",
       "chem    167\n",
       "math    115\n",
       "phy     105\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"./JEE/output/Jee_MAD_Scores+4.xlsx\")\n",
    "print(\"Total: \", sum(df['Score']))\n",
    "df.groupby('subject')['Score'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5feebd-9dc0-45c1-b319-fc6518d62ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funsearch",
   "language": "python",
   "name": "funsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
